{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnLOZhASlpYU",
        "colab_type": "code",
        "outputId": "8c4a6183-8286-4b6d-b6e7-cea6569a175f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.optimizers\n",
        "import tensorflow.keras.losses\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import json\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2D2bolj92Yv",
        "colab_type": "code",
        "outputId": "92be5794-4354-4fba-ac2d-dc1622718dda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NHa8LqG1PPL",
        "colab_type": "text"
      },
      "source": [
        "## Import and check data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ2csDtHhwjh",
        "colab_type": "code",
        "outputId": "42162c5b-5156-48c9-ba3e-1d9dcda4bb93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Mount drive to access files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK1-jvEHkhkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Paths\n",
        "train_y_path = \"/content/drive/My Drive/DD2424Files/train_y.npz\"\n",
        "val_y_path   = \"/content/drive/My Drive/DD2424Files/val_y.npz\"\n",
        "test_y_path  = \"/content/drive/My Drive/DD2424Files/test_y.npz\"\n",
        "\n",
        "train_X_path = \"/content/drive/My Drive/DD2424Files/train_X.npz\"\n",
        "val_X_path   = \"/content/drive/My Drive/DD2424Files/val_X.npz\"\n",
        "test_X_path  = \"/content/drive/My Drive/DD2424Files/test_X.npz\"\n",
        "\n",
        "train_X_mfcc_path = \"/content/drive/My Drive/DD2424Files/train_X_mfcc.npz\"\n",
        "val_X_mfcc_path   = \"/content/drive/My Drive/DD2424Files/val_X_mfcc.npz\"\n",
        "test_X_mfcc_path  = \"/content/drive/My Drive/DD2424Files/test_X_mfcc.npz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz42DCwnlNSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import training and validation data\n",
        "train_y = np.load(train_y_path)['arr_0']\n",
        "train_y = tf.one_hot(train_y, 8)\n",
        "\n",
        "val_y = np.load(val_y_path)['arr_0']\n",
        "val_y = tf.one_hot(val_y, 8)\n",
        "\n",
        "#train_X = tf.convert_to_tensor(np.expand_dims(np.load(train_X_path)['arr_0'], -1))\n",
        "#val_X = tf.convert_to_tensor(np.expand_dims(np.load(val_X_path)['arr_0'], -1))\n",
        "train_X_mfcc = np.load(train_X_mfcc_path)['arr_0']\n",
        "val_X_mfcc = np.load(val_X_mfcc_path)['arr_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TehQvMP_WmyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X_mfcc = np.load(train_X_mfcc_path)['arr_0']\n",
        "val_X_mfcc = np.load(val_X_mfcc_path)['arr_0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQL8X6LzdMUR",
        "colab_type": "code",
        "outputId": "f73f752a-0091-48c5-c06a-79cd69cea4c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Check the data shape\n",
        "print(train_X_mfcc.shape)\n",
        "print(val_X_mfcc.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6397, 140)\n",
            "(800, 140)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Uli9KOIl66I",
        "colab_type": "code",
        "outputId": "04905bdd-ee29-480e-ea75-37a9521d5f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# Assert assumptions about data\n",
        "\n",
        "MEL_BINS   = 96\n",
        "TIME_STEPS = 1405\n",
        "\n",
        "def check_data(X, y):\n",
        "  assert len(y.shape) == 2\n",
        "  assert len(X.shape) == 4\n",
        "  assert y.shape[0] == X.shape[0]\n",
        "  assert X.shape[1] == MEL_BINS\n",
        "  assert X.shape[2] == TIME_STEPS\n",
        "  assert X.shape[3] == 1\n",
        "\n",
        "check_data(val_X, val_y)\n",
        "check_data(train_X, train_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-865282d07e04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcheck_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mcheck_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb8UYWuu1xMj",
        "colab_type": "text"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwasJAxusnXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All model definitions\n",
        "\n",
        "def get_mlp_model():\n",
        "\n",
        "  mlp_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    #tf.keras.layers.Dense(128, activation='relu'),\n",
        "    #tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(8)\n",
        "  ])\n",
        "  return mlp_model\n",
        "\n",
        "#-------------------------------------------------------\n",
        "\n",
        "def cnn1_model(hyperparams):\n",
        "  NUMBER_FILTERS_1 = hyperparams.get(\"number_filters_1\")\n",
        "  KERNEL_SIZE = hyperparams.get(\"kernel_size_1\")\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(NUMBER_FILTERS_1, KERNEL_SIZE, padding = 'same', input_shape = (96, 1405, 1)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(8)                             \n",
        "  ])\n",
        "  return model\n",
        "\n",
        "#-------------------------------------------------------\n",
        "def cnn_k1c2_model(hyperparams):\n",
        "  h = hyperparams\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Input layer\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape = (96, 1405, 1)))\n",
        "\n",
        "  # Batch normalize\n",
        "  model.add(tf.keras.layers.BatchNormalization(axis = 1))\n",
        "\n",
        "  # Convolutional blocks\n",
        "  for i in range(len(h[\"number_filters\"])):\n",
        "    NUMBER_FILTERS = h[\"number_filters\"][i]\n",
        "    KERNEL_SIZE    = h[\"kernel_size\"][i]\n",
        "    POOLING_SIZE   = h[\"pooling_size\"][i]\n",
        "    DROP_OUT_SIZE  = h[\"dropout\"]\n",
        "\n",
        "    model.add(tf.keras.layers.Conv2D(NUMBER_FILTERS, KERNEL_SIZE, padding = 'same'))\n",
        "    model.add(tf.keras.layers.BatchNormalization(axis = 3))\n",
        "    model.add(tf.keras.layers.Activation(\"elu\"))\n",
        "    model.add(tf.keras.layers.MaxPool2D(POOLING_SIZE))\n",
        "    model.add(tf.keras.layers.Dropout(DROP_OUT_SIZE))\n",
        "\n",
        "\n",
        "  # Classifier block\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  for el in h[\"hidden_layers\"]:\n",
        "    model.add(tf.keras.layers.Dense(el))\n",
        "    model.add(tf.keras.layers.Dropout(h[\"drop_out_hidden\"]))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(8))\n",
        "\n",
        "  return model\n",
        "\n",
        "  #-----------------------------\n",
        "def mlp_model(hyperparams):\n",
        "\n",
        "  NO_HL = hyperparams[\"no_hl\"]\n",
        "  HL_WIDTH = hyperparams[\"hl_width\"]\n",
        "  DROPOUT = hyperparams[\"dropout\"]\n",
        "\n",
        "  mlp_model = tf.keras.models.Sequential()\n",
        "  mlp_model.add(tf.keras.layers.Flatten())\n",
        "  for i in range(NO_HL):\n",
        "    mlp_model.add(tf.keras.layers.Dense(HL_WIDTH, activation='elu'))\n",
        "    mlp_model.add(tf.keras.layers.Dropout(DROPOUT))\n",
        "  mlp_model.add(tf.keras.layers.Dense(8, activation = 'softmax'))\n",
        "\n",
        "  return mlp_model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eFYkHJrnPHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions for saving outputs\n",
        "\n",
        "def plot_history(history, val_string, loss_string):\n",
        "  # Plot results\n",
        "  plt.figure(1)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Acc')\n",
        "  plt.legend(['Accuracy', 'Validation accuracy'])\n",
        "  plt.title('Accuracy')\n",
        "  plt.ylim([0, 1])\n",
        "  plt.savefig(val_string, dpi = 300)\n",
        "  plt.close()\n",
        "\n",
        "  plt.figure(2)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend(['Loss', 'Validation loss'])\n",
        "  plt.title('Loss')\n",
        "  plt.savefig(loss_string, dpi = 300)\n",
        "  plt.close()\n",
        "\n",
        "def filename():\n",
        "\treturn \"S\" + str(datetime.datetime.now()).replace(':','.')[2:19]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IoPP5XEA30h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run an experiment\n",
        "\n",
        "def run_experiment(model, model_string, hyperparams):\n",
        "\n",
        "  file_location = \"/content/drive/My Drive/DD2424Files/Results/\" + filename() + \"/\"\n",
        "  os.mkdir(file_location)\n",
        "\n",
        "  # Compile model\n",
        "\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams.get(\"learning_rate\"), epsilon=hyperparams.get(\"epsilon\"))\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
        "  metrics = ['accuracy']\n",
        "\n",
        "  model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
        "\n",
        "  # Train model\n",
        "  BATCH_SIZE = hyperparams.get(\"batch_size\")\n",
        "  EPOCHS = hyperparams.get(\"epochs\")\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  save_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "      file_location + \"models_\" + model_string, monitor='val_accuracy', verbose=0, save_best_only=True,\n",
        "      save_weights_only=False, mode='max', save_freq='epoch')\n",
        "\n",
        "  #history = model.fit(train_X, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                          #validation_data=(val_X, val_y), verbose=1, callbacks = [save_callback])\n",
        "    \n",
        "  history = model.fit(train_X_mfcc, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "                          validation_data=(val_X_mfcc, val_y), verbose=1, callbacks = [save_callback])\n",
        "\n",
        "  # Save stuff\n",
        "  plot_history(history, file_location + \"acc_\" + model_string + \".png\", file_location + \"loss_\" + model_string + \".png\")\n",
        "  json.dump(history.history, open(file_location + \"history_\" + model_string, 'w'))\n",
        "  json.dump(hyperparams, open(file_location + \"hyperparams_\" + model_string, 'w'))\n",
        "\n",
        "  return history\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elKyRtkctwqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Experiment 1\n",
        "\n",
        "hyperparams = {\"number_filters_1\" : 15,\n",
        "               \"kernel_size_1\" : (1, 4),\n",
        "               \"epsilon\": 0.01, \n",
        "               \"learning_rate\": 0.01, \n",
        "               \"batch_size\": 32, \n",
        "               \"epochs\": 20}\n",
        "\n",
        "model_string = \"cnn15-fc8\"\n",
        "model = cnn1_model(hyperparams)\n",
        "\n",
        "history = run_experiment(model, model_string, hyperparams)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcQ9L_W9jy6Q",
        "colab_type": "text"
      },
      "source": [
        "## Experiment 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL99OFR973kZ",
        "colab_type": "code",
        "outputId": "4680f3f7-110d-4ac2-c561-567cb0dfdd05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Experiment 2\n",
        "\n",
        "hyperparams = {\"number_filters\" : [15, 15, 30, 30],\n",
        "               \"kernel_size\" : [(96, 4), (1,4), (1,4), (1,4)],\n",
        "               \"pooling_size\" : [(96,4), (1,5), (1,8), (1,8)],\n",
        "               \"hidden_layers\": [30,30],\n",
        "               \"epsilon\": 0.01, \n",
        "               \"learning_rate\": 0.01, \n",
        "               \"batch_size\": 32, \n",
        "               \"epochs\": 100,\n",
        "               \"dropout\" : 0,\n",
        "               \"drop_out_hidden\" : 0\n",
        "               }\n",
        "\n",
        "model_string = \"cnn_k2c1\"\n",
        "model = cnn_k1c2_model(hyperparams)\n",
        "\n",
        "history = run_experiment(model, model_string, hyperparams)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.8332 - accuracy: 0.3186WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 79s 394ms/step - loss: 1.8332 - accuracy: 0.3186 - val_loss: 2.2714 - val_accuracy: 0.2225\n",
            "Epoch 2/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.6329 - accuracy: 0.4041INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 385ms/step - loss: 1.6329 - accuracy: 0.4041 - val_loss: 2.1222 - val_accuracy: 0.2550\n",
            "Epoch 3/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.5572 - accuracy: 0.4472INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 385ms/step - loss: 1.5572 - accuracy: 0.4472 - val_loss: 2.1937 - val_accuracy: 0.3125\n",
            "Epoch 4/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.4674 - accuracy: 0.4801INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 1.4674 - accuracy: 0.4801 - val_loss: 2.1479 - val_accuracy: 0.3700\n",
            "Epoch 5/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.4232 - accuracy: 0.5026INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 1.4232 - accuracy: 0.5026 - val_loss: 1.6373 - val_accuracy: 0.4375\n",
            "Epoch 6/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 1.3974 - accuracy: 0.5071 - val_loss: 1.8626 - val_accuracy: 0.3812\n",
            "Epoch 7/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.3581 - accuracy: 0.5302 - val_loss: 2.1001 - val_accuracy: 0.3200\n",
            "Epoch 8/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.3246 - accuracy: 0.5388 - val_loss: 1.8488 - val_accuracy: 0.4212\n",
            "Epoch 9/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.3050 - accuracy: 0.5471INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 386ms/step - loss: 1.3050 - accuracy: 0.5471 - val_loss: 1.5796 - val_accuracy: 0.4725\n",
            "Epoch 10/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.2667 - accuracy: 0.5535 - val_loss: 1.6712 - val_accuracy: 0.4363\n",
            "Epoch 11/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.2588 - accuracy: 0.5621INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 387ms/step - loss: 1.2588 - accuracy: 0.5621 - val_loss: 1.5121 - val_accuracy: 0.4837\n",
            "Epoch 12/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.2267 - accuracy: 0.5762INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 385ms/step - loss: 1.2267 - accuracy: 0.5762 - val_loss: 1.5298 - val_accuracy: 0.5025\n",
            "Epoch 13/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.2228 - accuracy: 0.5742 - val_loss: 2.0002 - val_accuracy: 0.3338\n",
            "Epoch 14/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 1.1946 - accuracy: 0.5879 - val_loss: 2.1202 - val_accuracy: 0.3413\n",
            "Epoch 15/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.1772 - accuracy: 0.5873 - val_loss: 1.7333 - val_accuracy: 0.4325\n",
            "Epoch 16/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.1698 - accuracy: 0.5909 - val_loss: 1.7935 - val_accuracy: 0.4288\n",
            "Epoch 17/100\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 1.1405 - accuracy: 0.6001 - val_loss: 2.1002 - val_accuracy: 0.3750\n",
            "Epoch 18/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.1282 - accuracy: 0.6073 - val_loss: 2.4319 - val_accuracy: 0.3638\n",
            "Epoch 19/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 1.1488 - accuracy: 0.5976INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 387ms/step - loss: 1.1488 - accuracy: 0.5976 - val_loss: 1.5280 - val_accuracy: 0.5125\n",
            "Epoch 20/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 1.0944 - accuracy: 0.6181 - val_loss: 1.8174 - val_accuracy: 0.4437\n",
            "Epoch 21/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.0987 - accuracy: 0.6223 - val_loss: 1.8619 - val_accuracy: 0.4538\n",
            "Epoch 22/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 1.0689 - accuracy: 0.6225 - val_loss: 1.7932 - val_accuracy: 0.4487\n",
            "Epoch 23/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 1.0789 - accuracy: 0.6222 - val_loss: 1.7156 - val_accuracy: 0.4563\n",
            "Epoch 24/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 1.0518 - accuracy: 0.6292 - val_loss: 2.5781 - val_accuracy: 0.3462\n",
            "Epoch 25/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 1.0355 - accuracy: 0.6353 - val_loss: 1.6826 - val_accuracy: 0.4737\n",
            "Epoch 26/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 1.0194 - accuracy: 0.6390 - val_loss: 1.8978 - val_accuracy: 0.4250\n",
            "Epoch 27/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.9923 - accuracy: 0.6459 - val_loss: 1.7939 - val_accuracy: 0.4500\n",
            "Epoch 28/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.9806 - accuracy: 0.6578 - val_loss: 2.1878 - val_accuracy: 0.3975\n",
            "Epoch 29/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.9787 - accuracy: 0.6514 - val_loss: 1.7803 - val_accuracy: 0.4525\n",
            "Epoch 30/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.9559 - accuracy: 0.6631 - val_loss: 1.9500 - val_accuracy: 0.4150\n",
            "Epoch 31/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.9473 - accuracy: 0.6673 - val_loss: 1.7706 - val_accuracy: 0.4638\n",
            "Epoch 32/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.9496 - accuracy: 0.6606 - val_loss: 2.5858 - val_accuracy: 0.3237\n",
            "Epoch 33/100\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 0.9272 - accuracy: 0.6748 - val_loss: 2.3457 - val_accuracy: 0.3887\n",
            "Epoch 34/100\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.9017 - accuracy: 0.6788INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 13.13.18/models_cnn_k2c1/assets\n",
            "200/200 [==============================] - 77s 385ms/step - loss: 0.9017 - accuracy: 0.6788 - val_loss: 1.6105 - val_accuracy: 0.5238\n",
            "Epoch 35/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.9177 - accuracy: 0.6773 - val_loss: 2.1906 - val_accuracy: 0.3725\n",
            "Epoch 36/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.9109 - accuracy: 0.6763 - val_loss: 1.7905 - val_accuracy: 0.4863\n",
            "Epoch 37/100\n",
            "200/200 [==============================] - 73s 364ms/step - loss: 0.8734 - accuracy: 0.6966 - val_loss: 2.1588 - val_accuracy: 0.4087\n",
            "Epoch 38/100\n",
            "200/200 [==============================] - 73s 364ms/step - loss: 0.8647 - accuracy: 0.6952 - val_loss: 2.1321 - val_accuracy: 0.4100\n",
            "Epoch 39/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.8424 - accuracy: 0.7047 - val_loss: 1.8539 - val_accuracy: 0.4800\n",
            "Epoch 40/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.8641 - accuracy: 0.6925 - val_loss: 1.7654 - val_accuracy: 0.4363\n",
            "Epoch 41/100\n",
            "200/200 [==============================] - 73s 364ms/step - loss: 0.8545 - accuracy: 0.6974 - val_loss: 2.1302 - val_accuracy: 0.4600\n",
            "Epoch 42/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.8312 - accuracy: 0.7052 - val_loss: 1.8618 - val_accuracy: 0.4500\n",
            "Epoch 43/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.8308 - accuracy: 0.7020 - val_loss: 2.0893 - val_accuracy: 0.4425\n",
            "Epoch 44/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.8159 - accuracy: 0.7150 - val_loss: 2.1145 - val_accuracy: 0.4200\n",
            "Epoch 45/100\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 0.8087 - accuracy: 0.7178 - val_loss: 1.9755 - val_accuracy: 0.4462\n",
            "Epoch 46/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.8187 - accuracy: 0.7089 - val_loss: 1.8659 - val_accuracy: 0.4650\n",
            "Epoch 47/100\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 0.7778 - accuracy: 0.7225 - val_loss: 1.8837 - val_accuracy: 0.4688\n",
            "Epoch 48/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7902 - accuracy: 0.7180 - val_loss: 2.2461 - val_accuracy: 0.4387\n",
            "Epoch 49/100\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 0.7748 - accuracy: 0.7242 - val_loss: 1.8697 - val_accuracy: 0.5125\n",
            "Epoch 50/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7644 - accuracy: 0.7282 - val_loss: 1.9867 - val_accuracy: 0.4787\n",
            "Epoch 51/100\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 0.7507 - accuracy: 0.7336 - val_loss: 1.8768 - val_accuracy: 0.4988\n",
            "Epoch 52/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7474 - accuracy: 0.7374 - val_loss: 2.0517 - val_accuracy: 0.4638\n",
            "Epoch 53/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7483 - accuracy: 0.7349 - val_loss: 2.1241 - val_accuracy: 0.4175\n",
            "Epoch 54/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.7331 - accuracy: 0.7394 - val_loss: 2.0944 - val_accuracy: 0.4638\n",
            "Epoch 55/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7404 - accuracy: 0.7433 - val_loss: 2.1204 - val_accuracy: 0.4300\n",
            "Epoch 56/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7233 - accuracy: 0.7444 - val_loss: 2.1931 - val_accuracy: 0.4275\n",
            "Epoch 57/100\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 0.7068 - accuracy: 0.7450 - val_loss: 2.4519 - val_accuracy: 0.4450\n",
            "Epoch 58/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7004 - accuracy: 0.7580 - val_loss: 2.3818 - val_accuracy: 0.4300\n",
            "Epoch 59/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7177 - accuracy: 0.7443 - val_loss: 2.3327 - val_accuracy: 0.4263\n",
            "Epoch 60/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7013 - accuracy: 0.7527 - val_loss: 2.8173 - val_accuracy: 0.3438\n",
            "Epoch 61/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.7006 - accuracy: 0.7514 - val_loss: 2.0409 - val_accuracy: 0.4550\n",
            "Epoch 62/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6943 - accuracy: 0.7539 - val_loss: 2.4597 - val_accuracy: 0.4275\n",
            "Epoch 63/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6692 - accuracy: 0.7608 - val_loss: 2.0050 - val_accuracy: 0.5138\n",
            "Epoch 64/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6754 - accuracy: 0.7579 - val_loss: 2.1592 - val_accuracy: 0.4375\n",
            "Epoch 65/100\n",
            "200/200 [==============================] - 73s 367ms/step - loss: 0.6553 - accuracy: 0.7647 - val_loss: 2.4762 - val_accuracy: 0.4025\n",
            "Epoch 66/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6659 - accuracy: 0.7672 - val_loss: 2.6179 - val_accuracy: 0.3800\n",
            "Epoch 67/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6733 - accuracy: 0.7577 - val_loss: 1.8514 - val_accuracy: 0.5025\n",
            "Epoch 68/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6360 - accuracy: 0.7691 - val_loss: 2.7742 - val_accuracy: 0.4150\n",
            "Epoch 69/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6436 - accuracy: 0.7669 - val_loss: 2.4212 - val_accuracy: 0.4125\n",
            "Epoch 70/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6485 - accuracy: 0.7700 - val_loss: 2.5124 - val_accuracy: 0.4125\n",
            "Epoch 71/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6332 - accuracy: 0.7732 - val_loss: 2.5642 - val_accuracy: 0.4675\n",
            "Epoch 72/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6340 - accuracy: 0.7768 - val_loss: 2.3710 - val_accuracy: 0.4025\n",
            "Epoch 73/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6241 - accuracy: 0.7830 - val_loss: 2.7322 - val_accuracy: 0.3913\n",
            "Epoch 74/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.6288 - accuracy: 0.7744 - val_loss: 2.2545 - val_accuracy: 0.4350\n",
            "Epoch 75/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6112 - accuracy: 0.7813 - val_loss: 2.2426 - val_accuracy: 0.4588\n",
            "Epoch 76/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5851 - accuracy: 0.7905 - val_loss: 2.7991 - val_accuracy: 0.4412\n",
            "Epoch 77/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.6112 - accuracy: 0.7791 - val_loss: 2.3297 - val_accuracy: 0.4212\n",
            "Epoch 78/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5955 - accuracy: 0.7841 - val_loss: 3.0221 - val_accuracy: 0.4038\n",
            "Epoch 79/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5742 - accuracy: 0.7943 - val_loss: 2.5260 - val_accuracy: 0.4275\n",
            "Epoch 80/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5881 - accuracy: 0.7919 - val_loss: 2.9781 - val_accuracy: 0.3550\n",
            "Epoch 81/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5915 - accuracy: 0.7910 - val_loss: 2.5019 - val_accuracy: 0.4338\n",
            "Epoch 82/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5722 - accuracy: 0.7994 - val_loss: 2.3706 - val_accuracy: 0.4538\n",
            "Epoch 83/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5946 - accuracy: 0.7910 - val_loss: 2.3676 - val_accuracy: 0.4400\n",
            "Epoch 84/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.5816 - accuracy: 0.7913 - val_loss: 2.3603 - val_accuracy: 0.4588\n",
            "Epoch 85/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.5761 - accuracy: 0.7880 - val_loss: 2.4233 - val_accuracy: 0.4525\n",
            "Epoch 86/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5674 - accuracy: 0.7955 - val_loss: 2.7210 - val_accuracy: 0.4512\n",
            "Epoch 87/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5597 - accuracy: 0.7979 - val_loss: 2.3777 - val_accuracy: 0.4800\n",
            "Epoch 88/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.5411 - accuracy: 0.8073 - val_loss: 2.4076 - val_accuracy: 0.4625\n",
            "Epoch 89/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5589 - accuracy: 0.8004 - val_loss: 2.6430 - val_accuracy: 0.4187\n",
            "Epoch 90/100\n",
            "200/200 [==============================] - 73s 364ms/step - loss: 0.5505 - accuracy: 0.8002 - val_loss: 2.4738 - val_accuracy: 0.4212\n",
            "Epoch 91/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5466 - accuracy: 0.8110 - val_loss: 2.7488 - val_accuracy: 0.4325\n",
            "Epoch 92/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.5490 - accuracy: 0.8013 - val_loss: 2.4964 - val_accuracy: 0.4363\n",
            "Epoch 93/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5444 - accuracy: 0.8048 - val_loss: 2.7412 - val_accuracy: 0.4338\n",
            "Epoch 94/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.5739 - accuracy: 0.7904 - val_loss: 2.5580 - val_accuracy: 0.4075\n",
            "Epoch 95/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5365 - accuracy: 0.8130 - val_loss: 2.7326 - val_accuracy: 0.4238\n",
            "Epoch 96/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.5457 - accuracy: 0.8085 - val_loss: 2.7554 - val_accuracy: 0.4238\n",
            "Epoch 97/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.5344 - accuracy: 0.8090 - val_loss: 3.0651 - val_accuracy: 0.3475\n",
            "Epoch 98/100\n",
            "200/200 [==============================] - 73s 365ms/step - loss: 0.5433 - accuracy: 0.8026 - val_loss: 2.5011 - val_accuracy: 0.4550\n",
            "Epoch 99/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5254 - accuracy: 0.8130 - val_loss: 2.7842 - val_accuracy: 0.4075\n",
            "Epoch 100/100\n",
            "200/200 [==============================] - 73s 366ms/step - loss: 0.5321 - accuracy: 0.8077 - val_loss: 3.1217 - val_accuracy: 0.4325\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 96, 1405, 1)       384       \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 96, 1405, 15)      5775      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 96, 1405, 15)      60        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 96, 1405, 15)      0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 1, 351, 15)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1, 351, 15)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 1, 351, 15)        915       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 351, 15)        60        \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1, 351, 15)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 70, 15)         0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1, 70, 15)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 1, 70, 30)         1830      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1, 70, 30)         120       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1, 70, 30)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 8, 30)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1, 8, 30)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 1, 8, 30)          3630      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1, 8, 30)          120       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1, 8, 30)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 30)          0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1, 1, 30)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 30)                930       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 30)                930       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 30)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 248       \n",
            "=================================================================\n",
            "Total params: 15,002\n",
            "Trainable params: 14,630\n",
            "Non-trainable params: 372\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFwttMUyj3XM",
        "colab_type": "text"
      },
      "source": [
        "## Experiment 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quYaZtP5wWaY",
        "colab_type": "code",
        "outputId": "80cbeca6-de2e-4681-aeb6-f07f011f1c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "  # Experiment \n",
        "  \n",
        "  hyperparams = {\n",
        "      \"hl_width\" : 128,\n",
        "      \"no_hl\" : 10,\n",
        "      \"dropout\" : 0,\n",
        "      \"epsilon\": 0.01,\n",
        "      \"learning_rate\": 0.01,\n",
        "      \"batch_size\": 32,\n",
        "      \"epochs\": 150\n",
        "  }\n",
        "\n",
        "  model_string = \"mlp_10HL128\"\n",
        "  model = mlp_model(hyperparams)\n",
        "\n",
        "  history = run_experiment(model, model_string, hyperparams)\n",
        "  model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "192/200 [===========================>..] - ETA: 0s - loss: 2.1491 - accuracy: 0.1243INFO:tensorflow:Assets written to: /content/drive/My Drive/DD2424Files/Results/S20-05-12 17.52.08/models_mlp_10HL128/assets\n",
            "200/200 [==============================] - 4s 19ms/step - loss: 2.1484 - accuracy: 0.1251 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 2/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 3/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 4/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 5/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 6/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 7/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 8/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 9/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 10/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 11/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 12/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 13/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 14/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 15/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 16/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 17/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 18/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 19/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 20/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 21/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 22/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 23/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 24/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 25/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 26/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 27/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 28/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 29/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 30/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 31/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 32/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 33/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 34/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 35/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 36/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 37/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 38/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 39/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 40/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 41/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 42/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 43/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 44/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 45/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 46/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 47/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 48/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 49/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 50/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 51/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 52/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 53/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 54/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 55/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 56/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 57/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 58/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 59/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 60/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 61/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 62/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 63/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 64/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 65/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 66/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 67/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 68/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 69/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 70/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 71/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 72/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 73/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 74/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 75/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 76/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 77/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 78/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 79/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 80/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 81/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 82/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 83/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 84/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 85/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 86/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 87/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 88/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 89/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 90/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 91/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 92/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 93/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 94/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 95/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 96/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 97/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 98/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 99/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 100/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 101/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 102/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 103/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 104/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 105/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 106/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 107/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 108/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 109/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 110/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 111/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 112/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 113/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 114/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 115/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 116/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 117/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 118/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 119/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 120/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 121/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 122/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 123/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 124/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 125/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 126/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 127/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 128/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 129/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 130/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 131/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 132/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 133/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 134/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 135/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 136/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 137/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 138/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 139/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 140/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 141/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 142/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 143/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 144/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 145/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 146/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 147/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 148/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 149/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Epoch 150/150\n",
            "200/200 [==============================] - 1s 6ms/step - loss: 2.1491 - accuracy: 0.1249 - val_loss: 2.1490 - val_accuracy: 0.1250\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             multiple                  18048     \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_28 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             multiple                  16512     \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             multiple                  1032      \n",
            "=================================================================\n",
            "Total params: 167,688\n",
            "Trainable params: 167,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCyOhXJJjAxL",
        "colab_type": "text"
      },
      "source": [
        "## Try on testdata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpd-FttxG3O-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use test data on some models\n",
        "#test_X = tf.convert_to_tensor(np.expand_dims(np.load(test_X_path)['arr_0'], -1))\n",
        "test_X_mfcc = np.load(test_X_mfcc_path)['arr_0']\n",
        "test_y = np.load(test_y_path)['arr_0']\n",
        "test_y = tf.one_hot(test_y, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DA8Y2njfns6",
        "colab_type": "code",
        "outputId": "1b2b6a35-3b5b-4243-a7c0-13601c288e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# import model and evaluate on test data\n",
        "\n",
        "#date_and_time = \"20-05-12 00.28.22\"\n",
        "date_and_time = \"S20-05-12 17.38.19\"\n",
        "custom_name = \"mlp_1HL128\"\n",
        "\n",
        "model_location = \"/content/drive/My Drive/DD2424Files/Results/\" + date_and_time + \"/models_\" + custom_name\n",
        "\n",
        "my_model = tf.keras.models.load_model(model_location)\n",
        "\n",
        "\n",
        "my_model.evaluate(train_X_mfcc, train_y)\n",
        "my_model.evaluate(val_X_mfcc, val_y)\n",
        "my_model.evaluate(test_X_mfcc, test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 1s 3ms/step - loss: 1.6072 - accuracy: 0.4188\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 1.5743 - accuracy: 0.4812\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 1.9657 - accuracy: 0.3487\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.965691328048706, 0.3487499952316284]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}